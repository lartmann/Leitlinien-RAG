{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitlinien RAG-System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral API Initialisierung\n",
    "\n",
    "Zunächst wird die API von Mistral initialisiert, da das Sprachmodell für den Retrieval-Teil benötigt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "llm = ChatMistralAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadaten\n",
    "\n",
    "Die Metadaten werden aus der beim Scrapen erstellten PDF ausgelesen und so angepasst, dass sie für die weitere Verarbeitung bereit sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metadata.json\", \"r\") as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "    metadata['Registiernummer'] = metadata['Registiernummer'].str.replace(\"Registernummer \", \"\")\n",
    "    metadata['Federführende Fachgesellschaft(en)'] = metadata['Federführende Fachgesellschaft(en)'].str.split(\", \")\n",
    "    metadata['Adressaten'] = metadata['Adressaten'].str.split(\", \")\n",
    "    metadata['Patientenzielgruppe'] = metadata['Patientenzielgruppe'].str.split(\", \")\n",
    "    metadata['Versorgungsbereich'] = metadata['Versorgungsbereich'].str.split(\", \")\n",
    "    metadata['Schlüsselwörter'] = metadata['Schlüsselwörter'].str.split(\", \")\n",
    "    metadata['pdf_links'] = 'https://register.awmf.org' + metadata['pdf_links'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "    metadata.drop(columns=['Patienteninformation', 'Zielorientierung der Leitlinie', 'Gründe für die Themenwahl',  ], inplace=True)\n",
    "    metadata = metadata.to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung der PDFs\n",
    "\n",
    "Bevor die PDFs indexiert werden können, müssen sie zunächst vorbereitet werden. Hierfür wird die Bibliothek pdfplumber verwendet. Damit kann der Text extrahiert und die Seitenzahl in den Metadaten gespeichert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain.schema import Document  # or whatever you're using\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\", device=\"mps\")  # Apple Silicon acceleration\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"Remove extra whitespace and normalize newlines for better matching.\"\"\"\n",
    "    return ' '.join(text.replace(\"\\n\", \" \").split())\n",
    "\n",
    "def load_pdf_with_tables(path, metadata):\n",
    "    docs = []\n",
    "    previous_table = None\n",
    "    previous_header = None\n",
    "\n",
    "    identifier = path.split(\"/\")[-1].split(\".\")[0]\n",
    "    pdf_metadata = metadata.get(identifier, {})\n",
    "\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            raw_text = page.extract_text() or \"\"\n",
    "            text = normalize(raw_text)\n",
    "\n",
    "            full_text = text.strip()\n",
    "            \n",
    "            page_metadata = pdf_metadata.copy()\n",
    "            page_metadata.update({\"page\": i + 1})\n",
    "            docs.append(Document(page_content=full_text.strip(), metadata=page_metadata))\n",
    "\n",
    "    return docs\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexierung aller Leitlinien\n",
    "\n",
    "Die Leitlinien sind in Abschnitte von jeweils etwa 1.000 Token unterteilt. Die Registriernummern der indexierten Dateien werden in einer Textdatei gespeichert, um eine mehrfache Indexierung zu verhindern, falls die Indexierung unerwartet unterbrochen wird. Dateien, bei denen die Indexierung gescheitert ist, werden ebenfalls in einer Textdatei abgespeichert, um es später erneut versuchen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# ----------------------------\n",
    "# Document splitter setup\n",
    "# ----------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-large-instruct\",\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"guidelines\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"/Volumes/FESTPLATTE/sma_project/chroma_langchain_db\",\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Indexing loop\n",
    "# ----------------------------\n",
    "base_dir = \"/Volumes/FESTPLATTE/sma_project/guidelines/\"\n",
    "max_batch_size = 5000\n",
    "\n",
    "with open(\"indexed_guidelines.txt\", \"r\") as f:\n",
    "    indexed_ids = set(f.read().splitlines())  # use set for faster lookup\n",
    "\n",
    "for idd in metadata.keys():\n",
    "    if idd in indexed_ids:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        pdf_path = f\"{base_dir}/{idd}.pdf\"\n",
    "        docs = load_pdf_with_tables(pdf_path, metadata)\n",
    "\n",
    "        docs_split = splitter.split_documents(docs)\n",
    "        \n",
    "        # add complex metadata filtering\n",
    "        filtered_docs = [\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata=filter_complex_metadata([doc])[0].metadata\n",
    "            )\n",
    "            for doc in docs_split\n",
    "        ]\n",
    "        \n",
    "        texts = [doc.page_content for doc in filtered_docs]\n",
    "        metadatas = [doc.metadata for doc in filtered_docs]\n",
    "        \n",
    "        # creation of the embedding\n",
    "        embeddings_list = model.encode(texts, show_progress_bar=False, batch_size=8, convert_to_tensor=True).tolist()\n",
    "        \n",
    "        # chunking the embeddings to avoid memory issues\n",
    "        for b_i, batch in enumerate(chunk_list(embeddings_list, max_batch_size)):\n",
    "            vector_store._collection.add(\n",
    "                embeddings=batch,\n",
    "                documents=texts[:len(batch)],\n",
    "                metadatas=metadatas[:len(batch)],\n",
    "                ids=[f\"{idd}-{b_i}-{i}\" for i in range(len(batch))]\n",
    "            )\n",
    "\n",
    "        with open(\"indexed_guidelines.txt\", \"a\") as f:\n",
    "            f.write(f\"{idd}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {idd}: {e}\")\n",
    "        with open(\"failed_guidelines.txt\", \"a\") as f:\n",
    "            f.write(f\"{idd}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
